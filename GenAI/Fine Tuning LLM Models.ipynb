{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced2d416",
   "metadata": {},
   "source": [
    "# Fine tuning LLM models\n",
    "\n",
    "\n",
    "Information source: https://www.youtube.com/watch?v=iOdFUJiB0Zc&ab_channel=freeCodeCamp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4597591",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Simple definition: Conversion from higher memory format to a lower memory format.\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "Any LLM is a neural network model. These neural networks have parameters. We keep hearing about a 7 billion parameter model or 3 billion parameter models. \n",
    "\n",
    "\n",
    "What are these parameters?\n",
    "These parameters are the learned weights and biases in the network. These parameters are stored as FP32, taking 32 bits for each parameter value. \n",
    "\n",
    "Now, suppose we want to fine-tune one of these bigger LLMs. \n",
    "* Loading this model on a personal computer, edge device, mobile device or VRAM is not possible. The size of the model is huge.\n",
    "* We can try loading the model on cloud which provides higher capacity. But the cost is also higher for cloud services. \n",
    "\n",
    "How can we reduce the size of the model?\n",
    "We can convert from a high bit format like FP32 to a smaller size format, like FP16 (half precision) or 8 bit representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c18e1b",
   "metadata": {},
   "source": [
    "### What is quantization?\n",
    "\n",
    "Convert a bigger model and quantize it to a smaller model, so that it can be used for purposes like faster inference, edge device computing, mobile computing, fine tuning etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc59aadf",
   "metadata": {},
   "source": [
    "### Disadvantage\n",
    "\n",
    "When we quantize, since we decrease the precision at which we store the model parameters, there is loss of information and hence loss of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f1601b",
   "metadata": {},
   "source": [
    "## Caliberation\n",
    "\n",
    "Caliberation is the process of performing quantization.\n",
    "\n",
    "\n",
    "**Types of quantization**\n",
    "\n",
    "1. Symmetric Quantization\n",
    "2. Assymmetric Quantization\n",
    "\n",
    "**Symmetric Quantization**\n",
    "\n",
    "Suppose, we have a list of numbers in the range of 0-1000. And these numbers are evenly distributed. Assume, these numbers are stored in FP32 and we want to store them as unit8 (positive 8 bit integer). \n",
    "\n",
    "So, \n",
    "\n",
    "original list range (0, 1000)\n",
    "scaled down range (0, 255) - highest number 8 bits can store is 2^8 - 1.\n",
    "\n",
    "scale factor = 1000 - 0\n",
    "               _________  = 3.92\n",
    "               \n",
    "               255 - 0\n",
    "               \n",
    "\n",
    "Now, any number in original number when divided by 3.92 and rounded will give us uint8 representation.\n",
    "\n",
    "For e.g .250 will become, round(250/3.92) = round(63.775.....) = 64\n",
    "\n",
    "**Assymetric Quantization**\n",
    "\n",
    "What if the data is not symmetrically distributed, maybe right skewed etc.\n",
    "\n",
    "So,\n",
    "\n",
    "original list range (-20, 1000)\n",
    "scaled range (0, 255)\n",
    "\n",
    "scale factor = (1000 - (-20))/255 = 1020 / 255 = 4.0\n",
    "\n",
    "Now, if we use this to scale -20, it will be\n",
    "\n",
    "-20 / 4.0 = -5\n",
    "\n",
    "But -5, is not in range 0, 255. So we adapt a zero point, which is +5.\n",
    "\n",
    "\n",
    "\n",
    "Hence, for quantization we have 2 parameters. They are scale and zero point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014a406c",
   "metadata": {},
   "source": [
    "## Model of Quantization\n",
    "\n",
    "1. Post training quantization\n",
    "2. Quantization aware training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1714f28",
   "metadata": {},
   "source": [
    "# Additional Learnings\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=DtEq44FTPM4&ab_channel=CodeEmporium\n",
    "\n",
    "Why Batch Normalization?\n",
    "\n",
    "1. Increases training speed - It smoothens the optimization landscape significantly. The smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. The optmization plan landscape has consistent gradients or smoother gradients.\n",
    "\n",
    "2. Allow sub-optimal starts - They make initial weight less important. Suppose we don't have BatchNorm, then the optimization landscape can be wide and if we choose a point that is far from minimum, training may take larger number of iterations to reach minimum. Whereas, if optimization landscape was BatchNorm-ed, then the landscape is expected to be smoother and contained, and any starting weights would be similar steps away from minimum.\n",
    "\n",
    "3. Acts as a Regularizer (a little) - Think of regularizer in NN as dropouts. Dropouts introduce randomness in the learning process. Batch normalization does induce some regularization. \n",
    "\n",
    "### Mini-batch\n",
    "\n",
    "Number of training samples to consider, before updating weigths.\n",
    "\n",
    "### FP32 - How it looks in memory?\n",
    "\n",
    "Out of 32 bits,\n",
    "\n",
    "1. 1-bit is used for sign (+/-)\n",
    "2. 7-bit is used to store number before decimal \n",
    "3. 24-bit is used to store mantissa (number after decimal)\n",
    "\n",
    "Hence, for number 7.32, \n",
    "\n",
    "1-bit will be used to store +, 7 bits to store 7 and 24 bits will be used to store 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9048b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
