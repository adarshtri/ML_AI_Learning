{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e9fcad",
   "metadata": {},
   "source": [
    "# Fine tuning LLM models\n",
    "\n",
    "\n",
    "Information source: https://www.youtube.com/watch?v=iOdFUJiB0Zc&ab_channel=freeCodeCamp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c704619",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Simple definition: Conversion from higher memory format to a lower memory format.\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "Any LLM is a neural network model. These neural networks have parameters. We keep hearing about a 7 billion parameter model or 3 billion parameter models. \n",
    "\n",
    "\n",
    "What are these parameters?\n",
    "These parameters are the learned weights and biases in the network. These parameters are stored as FP32, taking 32 bits for each parameter value. \n",
    "\n",
    "Now, suppose we want to fine-tune one of these bigger LLMs. \n",
    "* Loading this model on a personal computer, edge device, mobile device or VRAM is not possible. The size of the model is huge.\n",
    "* We can try loading the model on cloud which provides higher capacity. But the cost is also higher for cloud services. \n",
    "\n",
    "How can we reduce the size of the model?\n",
    "We can convert from a high bit format like FP32 to a smaller size format, like FP16 (half precision) or 8 bit representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c5c1b5",
   "metadata": {},
   "source": [
    "### What is quantization?\n",
    "\n",
    "Convert a bigger model and quantize it to a smaller model, so that it can be used for purposes like faster inference, edge device computing, mobile computing, fine tuning etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42418257",
   "metadata": {},
   "source": [
    "### Disadvantage\n",
    "\n",
    "When we quantize, since we decrease the precision at which we store the model parameters, there is loss of information and hence loss of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da02bb",
   "metadata": {},
   "source": [
    "## Caliberation\n",
    "\n",
    "Caliberation is the process of performing quantization.\n",
    "\n",
    "\n",
    "**Types of quantization**\n",
    "\n",
    "1. Symmetric Quantization\n",
    "2. Assymmetric Quantization\n",
    "\n",
    "**Symmetric Quantization**\n",
    "\n",
    "Suppose, we have a list of numbers in the range of 0-1000. And these numbers are evenly distributed. Assume, these numbers are stored in FP32 and we want to store them as unit8 (positive 8 bit integer). \n",
    "\n",
    "So, \n",
    "\n",
    "original list range (0, 1000)\n",
    "scaled down range (0, 255) - highest number 8 bits can store is 2^8 - 1.\n",
    "\n",
    "scale factor = 1000 - 0\n",
    "               _________  = 3.92\n",
    "               \n",
    "               255 - 0\n",
    "               \n",
    "\n",
    "Now, any number in original number when divided by 3.92 and rounded will give us uint8 representation.\n",
    "\n",
    "For e.g .250 will become, round(250/3.92) = round(63.775.....) = 64\n",
    "\n",
    "**Assymetric Quantization**\n",
    "\n",
    "What if the data is not symmetrically distributed, maybe right skewed etc.\n",
    "\n",
    "So,\n",
    "\n",
    "original list range (-20, 1000)\n",
    "scaled range (0, 255)\n",
    "\n",
    "scale factor = (1000 - (-20))/255 = 1020 / 255 = 4.0\n",
    "\n",
    "Now, if we use this to scale -20, it will be\n",
    "\n",
    "-20 / 4.0 = -5\n",
    "\n",
    "But -5, is not in range 0, 255. So we adapt a zero point, which is +5.\n",
    "\n",
    "\n",
    "\n",
    "Hence, for quantization we have 2 parameters. They are scale and zero point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ed91b0",
   "metadata": {},
   "source": [
    "## Model of Quantization\n",
    "\n",
    "1. Post training quantization\n",
    "2. Quantization aware training\n",
    "\n",
    "\n",
    "**Post Training Quantization**\n",
    "\n",
    "In this case, we already have a pre-trained model. If we want to use this model for my use case, we perform caliberation and convert the model into quantized model. Then use the model.\n",
    "\n",
    "Problem: There is lot of information and hence accuracy.\n",
    "\n",
    "**Quantization Aware Training**\n",
    "\n",
    "In this case, we will take our trained model. Perform quantization, and then we perform fine tuning with our data to make the model more accurate and then quantize again. \n",
    "\n",
    "With this, we do loose accuracy but we do make improvements with fine turning with custom data to make up for loss of accuracy.\n",
    "\n",
    "\n",
    "\n",
    "**QAT is used in general. PTQ is not used.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea224d",
   "metadata": {},
   "source": [
    "## Lora and QLORA - Indepth Intuition\n",
    "\n",
    "\n",
    "It is specifically used in fine-tuning of LLM model. Whenever we have a pre-trained model, like chatgpt-4, this is a pre-trained model. It is trained on large volume of data. \n",
    "\n",
    "There are various ways of fine-tuning. \n",
    "\n",
    "The fine-tuning happens on the base model weights. \n",
    "\n",
    "1. Full parameter fine-tuning.\n",
    "2. Domain specific fine-tuning (e.g. for finance, sales etc)\n",
    "3. Specific task fine-tuning (task A, Task B, Task C etc)\n",
    "\n",
    "\n",
    "**Full parameter fine-tuning**\n",
    "\n",
    "1. Update all model weights\n",
    "2. Hardware resource constraint is a challenge.\n",
    "\n",
    "Downstream tasks become difficult, like model inference, model monitoring etc.\n",
    "\n",
    "To overcome this challenge, we will use Lora and QLORA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa4a0e",
   "metadata": {},
   "source": [
    "## LoRA (Low-Rank Adaptation) for LLM Fine-Tuning\n",
    "\n",
    "### 1. The Parameter Problem\n",
    "Large language models (LLMs) like GPT-3 or BERT have billions of parameters (weights). Fine-tuning all of these parameters for every new task is:\n",
    "- **Expensive**: It requires a lot of computational resources.\n",
    "- **Slow**: It takes a long time to train.\n",
    "- **Inefficient**: Storing a full version of the model for each task takes up a lot of space.\n",
    "\n",
    "LoRA offers a solution by freezing most of the model's parameters and fine-tuning only a small subset.\n",
    "\n",
    "### 2. Core Idea: Low-Rank Decomposition\n",
    "LoRA is based on **low-rank approximation**. Hereâ€™s the detailed breakdown:\n",
    "\n",
    "- **Weight matrices in LLMs**: In a deep learning model, the computations mainly involve multiplying an input vector by large weight matrices.\n",
    "  \n",
    "- **Low-rank assumption**: LoRA assumes the updates to these large weight matrices during fine-tuning can be expressed as the product of two smaller matrices. This drastically reduces the number of parameters being updated.\n",
    "\n",
    "- **How LoRA modifies weight matrices**:\n",
    "  - Instead of updating the full weight matrix \\( W \\), LoRA inserts two small matrices \\( A \\) and \\( B \\) into the model, so:\n",
    "  \n",
    "    \\[\n",
    "    W' = W + \\Delta W\n",
    "    \\]\n",
    "  \n",
    "    Where \\( \\Delta W = A \\times B \\).\n",
    "\n",
    "  - **Matrix sizes**:\n",
    "    - \\( A \\): \\( r \\times k \\) (restores dimensionality).\n",
    "    - \\( B \\): \\( d \\times r \\) (reduces dimensionality).\n",
    "    - \\( r \\) is much smaller than both \\( d \\) and \\( k \\), making \\( A \\times B \\) a low-rank approximation of the full update.\n",
    "\n",
    "### 3. Freezing the Original Weights\n",
    "- In LoRA, the original weight matrix \\( W \\) is **frozen**. Only the newly introduced low-rank matrices \\( A \\) and \\( B \\) are updated during fine-tuning.\n",
    "\n",
    "- This means you update only a small portion of the model, which saves both time and memory.\n",
    "\n",
    "### 4. How LoRA is Applied\n",
    "- LoRA is typically applied to specific layers in the model, such as **attention layers** in transformers, where matrix multiplications are the most computationally expensive.\n",
    "\n",
    "- When input data passes through the model:\n",
    "  - It is first multiplied by the original weight matrix \\( W \\) (frozen).\n",
    "  - In parallel, it passes through the low-rank matrices \\( B \\) and \\( A \\), and these matrices are updated during fine-tuning.\n",
    "  - The outputs from both operations are summed: \\( W \\times \\text{input} + (A \\times (B \\times \\text{input})) \\).\n",
    "\n",
    "### 5. Advantages of LoRA\n",
    "- **Efficiency**: Only small matrices \\( A \\) and \\( B \\) are fine-tuned, which reduces memory and computational overhead.\n",
    "- **Modularity**: You can adapt a single model to multiple tasks by swapping in different LoRA modules without retraining the entire model.\n",
    "- **Minimal Performance Impact**: Despite fine-tuning fewer parameters, LoRA achieves performance close to traditional fine-tuning.\n",
    "\n",
    "### 6. Visualization\n",
    "Imagine your model is a large building. Instead of rebuilding the whole structure for each new task, LoRA adds small adjustment blocks (matrices \\( A \\) and \\( B \\)) to the building. These adjustments are enough to fine-tune the model without altering the entire foundation.\n",
    "\n",
    "### 7. Use in Transformers\n",
    "LoRA is particularly effective in transformer models, as the attention layers involve large matrix multiplications. By applying LoRA only to these layers, fine-tuning becomes much more efficient.\n",
    "\n",
    "### Summary\n",
    "1. **Start** with a large pre-trained model with frozen weights.\n",
    "2. **Inject** small matrices \\( A \\) and \\( B \\) into specific layers.\n",
    "3. **Fine-tune** only the small matrices while keeping the original model frozen.\n",
    "4. **Use** the adapted model for your new task with minimal additional parameters.\n",
    "\n",
    "LoRA allows you to fine-tune large models efficiently while saving both time and memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a329d61",
   "metadata": {},
   "source": [
    "# Additional Learnings\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Source: https://www.youtube.com/watch?v=DtEq44FTPM4&ab_channel=CodeEmporium\n",
    "\n",
    "Why Batch Normalization?\n",
    "\n",
    "1. Increases training speed - It smoothens the optimization landscape significantly. The smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. The optmization plan landscape has consistent gradients or smoother gradients.\n",
    "\n",
    "2. Allow sub-optimal starts - They make initial weight less important. Suppose we don't have BatchNorm, then the optimization landscape can be wide and if we choose a point that is far from minimum, training may take larger number of iterations to reach minimum. Whereas, if optimization landscape was BatchNorm-ed, then the landscape is expected to be smoother and contained, and any starting weights would be similar steps away from minimum.\n",
    "\n",
    "3. Acts as a Regularizer (a little) - Think of regularizer in NN as dropouts. Dropouts introduce randomness in the learning process. Batch normalization does induce some regularization. \n",
    "\n",
    "### Mini-batch\n",
    "\n",
    "Number of training samples to consider, before updating weigths.\n",
    "\n",
    "### FP32 - How it looks in memory?\n",
    "\n",
    "Out of 32 bits,\n",
    "\n",
    "1. 1-bit is used for sign (+/-)\n",
    "2. 7-bit is used to store number before decimal \n",
    "3. 24-bit is used to store mantissa (number after decimal)\n",
    "\n",
    "Hence, for number 7.32, \n",
    "\n",
    "1-bit will be used to store +, 7 bits to store 7 and 24 bits will be used to store 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd161193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
